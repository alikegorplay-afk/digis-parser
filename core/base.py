import asyncio
import random

from urllib.parse import urljoin

import aiohttp
from loguru import logger
from bs4 import Tag, BeautifulSoup
from fake_headers import Headers


class BaseParser:
    def __init__(self, session: aiohttp.ClientSession, base_url: str, parse_engine: str = 'html.parser', *, max_workers: int = 5, sleep_time: int = 3):
        self._session = session
        self._base_url = base_url
        self.parse_engine = parse_engine
        self.sleep_time = sleep_time
        
        self.semaphore = asyncio.Semaphore(max_workers)
        self.headers = Headers(
            browser="chrome",
            os="win",
            headers=True
        )
    
    def _safe_extract_url(self, tag: Tag, attr: str):
        try:
            if url := tag.get(attr):
                return urljoin(self._base_url, url)
            else:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–Ω–∏–∏ URL: {e}")
    
    async def _fetch(self, url: str, *args, **kwargs) -> BeautifulSoup | None:
        async with self.semaphore:
            delay = self.sleep_time + random.uniform(-0.5, 1.0)
            await asyncio.sleep(max(2.0, delay))  # –º–∏–Ω–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã
            
            for attempt in range(1, 4):
                try:
                    if attempt > 1:
                        wait_time = self.sleep_time * (2 ** (attempt - 1)) + random.uniform(1, 5)
                        logger.info(f"–ñ–¥–µ–º {wait_time:.1f} —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π")
                        await asyncio.sleep(wait_time)
                    
                    logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ #{attempt}: {url}")
                    async with self._session.get(
                        url, 
                        headers=self._get_headers(),
                        timeout=aiohttp.ClientTimeout(total=30),
                        *args, **kwargs
                    ) as response:
                        if response.status == 429:
                            logger.warning("üö® Rate limit! –î–µ–ª–∞–µ–º –¥–ª–∏–Ω–Ω—É—é –ø–∞—É–∑—É")
                            await asyncio.sleep(60)
                            continue
                        
                        elif response.status == 403:
                            logger.error("üíÄ –ü–æ–ª–Ω—ã–π –±–∞–Ω! –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è")
                            return 
                        
                        response.raise_for_status()
                        logger.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {url}")
                        return BeautifulSoup(await response.text(), self.parse_engine)
                        
                except aiohttp.ClientResponseError as e:
                    if 500 <= e.status < 600:
                        logger.warning(f"üîß –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {e.status}, –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞...")
                        continue
                    elif e.status == 404:
                        logger.warning(f"‚ùå 404: {url}")
                        return None
                    else:
                        logger.warning(f"‚ö†Ô∏è –ö–æ–¥ {e.status}: {url}")
                        continue
                        
                except asyncio.TimeoutError:
                    logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø–æ–ø—ã—Ç–∫–∞ #{attempt}")
                    continue
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {type(e).__name__}")
                    continue
            
            logger.error(f"üö´ –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è: {url}")
            return None

    def _get_headers(self):
        headers = self.headers.generate()
        headers['Referer'] = self._base_url
        headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        return headers
 